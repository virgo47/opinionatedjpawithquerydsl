# Questionable parts {#ch-questionable-parts}

The JPA, as any ORM, is not without its drawbacks. Firstly, it is complex -- much deeper than
developers realize when they approach it. Secondly, it is not a perfect abstraction. The more you
want to play it as a perfect abstraction, the worse it probably gets in marginal cases.
And the margin is not that thin. You may solve 80% cases easily, but there are still 20% of hard
cases where you go around your ORM, write native SQL, etc. If you try to avoid it you'll probably
suffer more than if you accepted it.

You can't just stay on the JPA level, even for cases where ORM works well for you. There are
some details you should know about the provider you use. For instance, let's say you have an entity
with auto-generated identifier based on IDENTITY (or AUTO_INCREMENT) column. You call `persist`
on it and later you want to use its ID somewhere. And it doesn't work, because you're using
EclipseLink and you didn't call `flush` to actually execute that INSERT. Without it the provider
cannot know what value for ID it should use. Maybe your usage of ID value was not the right
*ORM way*, maybe you should have use the whole entity somewhere, but the point is that if you do
the same with Hibernate, it would work. You simply cannot assume that the ID is set.[^demoid]

[^demoid]: You can see this demonstrated in `examples/querydsl-basic` if you run:
    `mvn test-compile exec:java -Dexec.mainClass="tests.GeneratedIdSettingDemo"`


## Lazy on basic and *to-one* fields {#lazy-problems}

While you can map these fields as lazy, the behaviour is actually not guaranteed according
to the JPA specification ([[JPspec](#bib-jpspec)]). Its section 2.2, page 25, states:

{icon=quote-right}
G> If property access is used and lazy fetching is specified, portable applications should not
G> directly access the entity state underlying the property methods of managed instances until
G> after it has been fetched by the persistence provider.

And the attached footnote adds:

{icon=quote-right}
G> Lazy fetching is a hint to the persistence provider and can be specified by means of the
G> `Basic`, `OneToOne`, `OneToMany`, `ManyToOne`, `ManyToMany`, and `ElementCollection` annotations
G> and their XML equivalents.

While ORMs generally have no problem to make collections lazy (e.g. both *to-many* annotations),
for *to-one* mappings this gets more complicated. [[PoEAA](#bib-poeaa)] offers couple of solutions
for *lazy load* pattern: *lazy initialization*, *virtual proxy*, *value holder*, and *ghost*. Not
all are usable for *to-one* mappings.

The essential trouble is that such a field contains an entity directly. There is no indirection
like a collection, that can provide lazy implementation, in case of *to-many* mappings. JPA does
not offer any generic solution for indirect *value holder*. *Virtual proxy* would require some
interface to implement, or bytecode manipulation on the target class, *ghost* would definitely
require bytecode manipulation on the target class, and *lazy initialization* would require
bytecode manipulation, or at least some special implementation, on the source class. JPA design
neither offers any reasonable way how to introduce this indirection without advanced auto-magic
solutions nor ways how to do it explicitly in a way a programmer can control.

Removing *to-one* mappings and replace them with raw foreign key values is currently not possible
with pure JPA even though *JPA 2.1* brought `ON` clause for JPQL but it does not allow root
entities in `JOIN`s. We will expand on this in the chapter
[Troubles with *to-one* relationships](#ch-to-one-troubles).


## Generated updates

Programmer using JPA should see the *object* side of the ORM mapping. This means that an object
is also the level of granularity on which ORM works. If you change a single attribute on an object
ORM can simply generate full update for all columns (except for those marked `updatable = false`,
of course). This by itself is probably not such a big deal performance-wise, but if nothing else
it makes SQL debug output less useful for checking what really changed.

I'd not even expect ORM to eliminate the column from update when it's equal, I'd rather expect
them to include it only when it was set. But we are already in the domain of ORM auto-magic (again)
as they somehow have to know what has changed. Our entities are typically enhanced somehow, either
during the build of a project, or during class-loading. It would probably be more complex to store
touched columns instead of marking the whole entity as "dirty".

To be concrete, EclipseLink out of the box updates only modified attributes/columns, while
Hibernate updates all of them except for ID which is part of the `WHERE` clause.[^el-hib-upd]
We may even argue what is better and when. If you load an object in some state, change a single
column and then commit the transaction, do we really want to follow the changes per attribute or do
we expect the whole object to be "transferred" into the database as-is at the moment of the commit?
If we don't squeeze performance and our transactions are short-lived (and they better are for most
common applications) there is virtually no difference from the consistency point either.

[^el-hib-upd]: Run the code from [this GitHub example](https://github.com/virgo47/opinionatedjpawithquerydsl/blob/master/examples/querydsl-basic/src/test/java/tests/PartialUpdateDemo.java)
    and see the console output.

All in all, this is just a minor annoyance when you're used to log generated queries, typically
during development, and you simply cannot see what changed among dozens of columns. For this --
and for the cases when you want to change many entities at once in the same way -- you can use
`UPDATE` clause, also known as *bulk update*. But these tend to interfere with caches and with
persistence context. We will talk about that in the next chapter.


## Unit of work vs queries

JPA without direct SQL-like capabilities (that is JPQL) would be very limited. Sure, there are
projects you can happily sail through with queries based on criteria API only, but those are the
easy ones. I remember a project, it was an important budgeting system with hierarchical
organizational structure with thousands of organizations. There were budget items and limits for
them with multiple categories, each of them being hierarchical too. When we needed to recalculate
some items for some category (possibly using wildcards) we loaded these items and then perform
the tasks in memory.

Sometimes it must have been done, rules were simply too complicated for an update. But sometimes
it didn't have to be this way. When the user approved the budget for an organization (and all its
sub-units) we merely needed to set a flag on all these items. That's what you can do with a bulk
update. `UPDATE` and `DELETE` clauses were in the JPA specification since day zero, with the latest
*JPA 2.1* you can do this not only in JPQL, but also in Criteria API.[^capiq]

[^capiq]: We will, however, prefer more expressive Querydsl that generates JPQL.

When you can formulate simple bulk update (you know what to set and `WHERE` to set it) you can
gain massive performance boost. Instead of iterating and generating N updates you just send a
single SQL to the database and that way you can go down from a cycle taking minutes to an operation
taking seconds. But there is one big "but" related to the persistence context (entity manager).
If you have the entities to be affected by the bulk update in your persistence context, they
will not be touched at all. Bulk updates go around entity manager's "cache" for unit of work, which
means you should not mix bulk updates with modification of entities attached to the persistence
context, unless these are completely separate entities. In general, I try to avoid any complex
logic with attached entities after I execute bulk update/delete -- and typically the scenario does
not require it anyway.

To demonstrate the problem with a snippet of code:

{title="BulkUpdateVsPersistenceContext.java", lang=java}
~~~
System.out.println("dog.name = " + dog.getName()); // Rex

new JPAUpdateClause(em, QDog.dog)
  .set(QDog.dog.name, "Dex")
  .execute();

dog = em.find(Dog.class, 1); // find does not do much here
System.out.println("dog.name = " + dog.getName()); // still Rex

em.refresh(dog); // this reads the real data now
System.out.println("after refresh: dog.name = " +
  dog.getName()); // Dex
~~~

The same problem applies to JPQL queries which happen after the changes to the entities within
a transaction on the current persistence context. Here the behaviour is controlled by entity
manager's flush mode and it defaults to `FlushModeType.AUTO`.[^flushmode] Flush mode `AUTO`
enforces the persistence context to flush all updates into the database before executing the query.
But with flush mode `COMMIT` you'd get to inconsistencies just like in the scenario with bulk
update. Obviously, flushing the changes is a reasonable option -- you'd flush it sooner or later
anyway. Bulk update scenario, on the other hand, requires us to refresh attached entities which
is much more disruptive and also costly.

[^flushmode]: See [[JPspec](#bib-jpspec)], section 3.10.8.


## You can't escape SQL and relation model {#cant-escape-sql}

Somewhere under cover there is SQL lurking and we better know how it works. We will tackle this
topic with more passion in the second part of the book -- for now we will just demonstrate that
not knowing does not work. Imagine we want a list of all dog owners, and because there is many
of them we want to paginate it. This is 101 of any enterprise application. In JPA we can use
methods `setFirstResult` and `setMaxResults` on `Query` object which corresponds to SQL `OFFSET`
and `LIMIT` clauses.[^pagstand]

[^pagstand]: In many popular databases that is. Standard SQL pagination is quite young and I doubt
    database vendors agreed to implement it.

Let's have a model situation with the following owners (and their dogs): Alan (with dogs Alan,
Beastie and Cessna), Charlie (no dog), Joe (with Rex) and Mike (with Lassie and Dunco). If you
query for the first two owners ordered by name -- pagination without order doesn't make sense --
you'll get Alan and Charlie. However, imagine you want to display names of their dogs in each row.
If you join dogs too, you'll just get Alan twice, courtesy of his many dogs. You may select without
the join and then select dogs for each row, which is our infamous N+1 select problem. This may not
be a big deal for a page of 2, but for 30 or 100 you can see the difference. We will talk about
this particular problem later [in the chapter about N+1](#to-many-paginating).

These are the effects of the underlying relational model and you cannot escape them. It's not
difficult to deal with them if you accept the relational world underneath. If you fight it, it
fights back.


## Additional layer

Reasoning about common structured, procedural code is quite simple for simple scenarios. We add
higher-level concepts and abstractions to deal with ever more complex problems. When you use JDBC
you know exactly where in the code your SQL is sent to the database, it's easy to control it,
debug it, monitor it, etc. With JPA we are one level higher. You still can try to measure
performance of your queries -- after all typical query is executed where you call it -- but there
are some twists.

First, it can be cached in a query cache -- which may be good if it provides correct results -- but
it also significantly distorts any performance measurement. JPA layer itself takes some time. Query
has to be parsed (add Querydsl serialization to it when used) and entities created and registered
with the persistence context, so they are *managed* as expected. This distorts the result for the
worse, not to mention that for big results it may trigger some additional GC that plain JDBC would
not have.

Best bet is to monitor performance of the SQL on the server itself. Most decent RDBMS provide
some capabilities in this aspect. You can also use JDBC proxy driver that wraps the real one and
performs some logging on the way. Maybe your ORM provides this to a degree, at least in the logs
if nowhere else. This may not be easy to process, but it's still better than no visibility at all.
More sophisticated system may provide nice measurement, but can also add performance penalty --
which perhaps doesn't affect the measured results too much, but it can still affect the overall
application performance. Of course, monitoring overhead is not related only to JPA, we would get
it using just plain JDBC as well.

I will not cover monitoring topics in the book -- they are natural problems with any framework,
although we can argue that access to RDBMS is kinda critical. Unit of work pattern causes real DB
work to happen somewhere else than the domain code would indicate. For simple CRUD-like scenarios
it's not a problem, not even from performance perspective (mostly). For complex scenarios, for
which the pattern was designed in the first place, we may need to revisit what we send to the
database if we encounter performance issues. This may also affect our domain. Maybe there are
clean answers for this, but I don't know them. I typically rather tune down how I use the JPA.

All in all, the fact that JPA hides some complexity and adds another set of it is a natural aspect
of any layer you add to your application -- especially the technological one. This is probably one
of the least questionable parts of JPA. You either want it and accept its "additional layer-ness"
or choose not to use it. Know that when you discover any problem you will likely have to
deal with the complexity of the whole stack.


## Big unit of work

I believe that *unit of work* pattern is really neat, especially when you have support from ORM
tools. But there are legitimate cases when we can run into troubles because the context is simply
too big. This may easily happen with complicated business scenarios and it may cause no problem.
Often, though, users may see the problem. Request takes too
long or nightly scheduled task is still running late in the morning. Code looks good, it's ORM-ish
as it can be -- it's just slow.

You can monitor or debug how many objects are managed, often you can see effects on the heap. When
this happens something has to change, obviously. Sometimes you can deal with the problem within
your domain model and let ORM shield you from the database. Sometimes it's not possible and you
have to let relational world leak into your code.

D> ### How object-oriented is JPQL?
D>
D> I've seen advices how to perform some updates or deletes in "more object-oriented manner" -- and
D> they were probably right from that perspective. But that often pulls a lot of unnecessary data
D> into memory to perform something that can be expressed easily with JPQL. If JPQL is part of JPA,
D> just like other QLs are parts of concrete ORM solutions, then it arguably *is* part of the
D> abstraction. Let's examine various options in the following example.

Let's say we have some test cases creating some dog and breed objects. In ideal case we would
delete all of them between tests, but as it happens, we are working on the database that contains
some fixed set of dogs and breeds as well (don't ask). So we mark our test breeds with 'TEST'
description. Dog creation is part of tested code, but we know they will be of our testing breeds.
To delete all the dogs created during the test we may then write:

~~~
delete from Dog d where d.breed.description = 'TEST'
~~~

That's pretty clear JPQL. Besides the fact that it [fails on Hibernate](https://hibernate.atlassian.net/browse/HHH-9711)
it does not touch our persistence context at all and does its job. We can do the same with
subquery (works for Hibernate as well) -- or we can fetch testing breeds into a list (they
are now managed by the persistence context) and then execute JPQL like this:

~~~
delete from Dog d where d.breed in :breeds
~~~

Here `breeds` would be parameter with a list containing the testing breeds as its value. We may
fetch a plain list of breed.id instead, this does not get managed, takes less memory and pulls
less data from the database with the same effect, we just say `where d.breed.id in :breedIds`
instead -- if supported like this by your ORM, but it's definitely OK with JPA. I've heard
arguments that this is less object-oriented. I was like "what?"

Finally, what you can do is start with fetching the testing breeds and then fetch all the
dogs with these breeds and call `em.remove(dog)` in a cycle. I hope this, object-oriented as it
is, is considered a bit of a stretch even by OO programmers. But I saw it in teams where JPQL
and bulk updates were not very popular (read also as "not part of the knowledge base").

W> In most cases the persistence context is short-lived, so even when a lot of data flows through,
W> it will probably not be a big deal for the
W> [garbage collector](https://en.wikipedia.org/wiki/Garbage_collection_%28computer_science%29) --
W> unless you seriously overflow young generation. But it will affect CPU load, network bandwidth
W> -- as the "object-oriented" approach likely goes to the database multiple times -- and/or
W> caches.
W>
W> If you cache a lot, it will affect application's memory footprint and garbage collector, and if
W> the cache is distributed it may still trigger network roundtrips. These may or may not be
W> quicker than the database access, depending on how much you can get by id; to re-hydrate a
W> cached entity should be faster than to fetch it from the database. I hope you can see a lot of
W> trade-offs you *should* think about when you choose any more sophisticated path -- this includes
W> paths that seem easy at their beginning only because they are covered with a veil of auto-magic
W> solutions. We will get to this when we talk about [Vietnam of Computer Science](#vietnam).

Persistence context (unit of work) has a lot to do when the transaction is committed. It needs to
be flushed which means it needs to check all the *managed* (or attached) entities, and figure out
whether they are changed or not (dirty checking). How the dirty checking is performed is beyond the
scope of this book, typically some bytecode manipulation is used. The problem is that this takes
longer when the persistence context is big. When it's big for a reason, then you have to do what
you have to do, but when it's big because we didn't harness bulk updates/deletes, than it's simply
wasteful. Often it also takes a lot of code when a simple JPQL says the same. I don't accept an
answer that reading JPQL or SQL is hard. If we use RDBMS we *must* know it reasonably well.

D> ### Do I need transactions for reading?
D>
D> This is a tricky one. Maybe not, but I encountered strange things happening when use cases
D> loaded stale data. This happened when we used Spring and its `@Transactional` annotation.
D> Without it I didn't see the changes from previously finished transactions. There was no race
D> condition involved, it was all nice slow clicking in a CRUD-like web application. Adding
D> `@Transactional` to the service that reads the data "magically" fixed it. I'm sorry I didn't
D> have time to dig deeper than, but I remember it vividly. So just know, it may happen.
D>
D> But if you make read-only scenario transactional you add all the burden we've just described
D> before. Spring offers attribute on the annotation to mark the transaction as read-only. This
D> disables dirty checking on the persistence context which makes sense. Unfortunately, if you
D> accidentally use some modification SQL via bulk update/delete or even going down to JDBC, it
D> may get committed, as not all JDBC drivers respect `connection.setReadOnly(true)`. While it's
D> an obvious bug (or missing feature, whatever), marking transaction as read-only does *not*
D> guarantee that nothing gets committed. It still says clearly your intention and it *should*
D> bring some time-saving on the *unit of work* level. Unfortunately, again, this does not work
D> universally, not even on the ORM level. Hibernate's session does not get flushed, but I'm
D> pretty sure EclipseLink flushes updates to the database on the JPA level and does not set
D> JDBC connection to [read-only either](https://jira.spring.io/browse/SPR-7891).
D>
D> I don't know how to mark transaction as read-only with JTA `@Transactional` annotation.
D>
D> I highly recommend reading older but still very relevant article [Transaction strategies:
D> Understanding transaction pitfalls](http://www.ibm.com/developerworks/java/library/j-ts1/)

Sometimes bulk update is not a feasible option, but you still want to loop through some
result, modify each entity without interacting with others and flush it (or batch the update if
possible). This sounds similar to the missing "cursor-like result set streaming" mentioned
in the section covering [missing features](#missing-from-orm) some ORM providers do have --
although that case covers read-only scenarios without any interaction with persistence context.
If you want to do something on each single entity as you loop through the results and you hit
a performance problems (mostly related to memory) you may try to load smaller chunks of the data
and after processing each chunk you flush the persistence context and also clear it with
`EntityManager.clear()`.

D> ### Flush and clear?!
D>
D> It is generally not recommended to call `flush()` or `clear()` explicitly as it does not follow
D> *unit of work* pattern and often also hurts the performance. You may read about this (and other)
D> anti-pattern in [this article](http://www.developerfusion.com/article/84945/flush-and-clear-or-mapping-antipatterns/)
D> (section *Anti-pattern: Flush and Clear*). But even that section mentions flush/clear as
D> useful when processing a big batch. Maybe JPA should have some cleaner abstraction for this,
D> but these are not typical scenarios and flush/clear is there when we need it.


## JPA alternatives?

When JPA suddenly stands in your way instead of helping you, you can still fallback gracefully
to JDBC level. Personally I don't mix these within a single scenario, but you can. For that you
have to know what provider you use, unwrap its concrete implementation of `EntityManager` and ask
it for a `java.sql.Connection` in non-portable manner. When I don't mix scenarios, I simply ask
Spring (or possibly other container) to inject underlying `javax.sql.DataSource` and then I can
access `Connection` without using JPA at all. Talking about Spring, I definitely go for their
`JdbcTemplate` to avoid all the JDBC boilerplate. Otherwise I prefer JPA for all the reasons
mentioned in the [Good Parts](#ch-good-parts).

We've lightly compared JPA with concrete ORMs already, but they are still the same concept -- it
is much more fun to compare it with something else altogether, let's say a different language --
like [Groovy](http://www.groovy-lang.org/). We're still firmly on JVM although it's not very likely
to do your persistence in Groovy and the rest of the application in Java. Firstly, Groovy language
also has its ORM. It's called [GORM](https://grails.github.io/grails-data-mapping/latest/) and
while not built into the core project it is part of [Grails framework](https://grails.org/). I
don't have any experience with it, but I don't expect radical paradigm shift as it uses Hibernate
ORM to access RDBMS (although it supports also No-SQL solutions). Knowing Groovy I'm sure it brings
some fun into the mix, but it still is ORM.

I often use core Groovy [support for relation databases](http://www.groovy-lang.org/databases.html)
and I really like it. It is no ORM, but it makes working with database really easy compared to
JDBC. I readily use it to automate data population as it is much more expressive than SQL
statements -- you mostly just use syntax based on Groovy Maps. With little support code you create
helper insert/update methods that can provide reasonable defaults for columns you don't want to
specify every time or insert whole aggregates (master-slave table structures). It's convenient to
assign returned auto-increment primary keys into Groovy variables and use them as foreign keys
where needed. It's also very easy to create repetitive data in a loop.

I use this basic database support in Groovy even on projects where I already have entities in JPA,
but for whatever reason I don't want to use them. You actually don't need to map anything into
objects, mostly couple of methods will do. Sure you have to rename columns in the code when you
refactor, but column names are hidden in just a couple places, often just a single one. Bottom
line? Very convenient, natural type conversion (although not perfect, mind you), little to no
boilerplate code and it all plays nicely with Groovy syntax. It definitely doesn't bring so much
negative passion with it as JPA does -- perhaps because the level of sophistication is not so high.
But in many cases simple solutions are more than enough.